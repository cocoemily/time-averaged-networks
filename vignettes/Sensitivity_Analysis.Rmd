---
title: "Sensitivity Analysis"
author: "Andrew Gillreath-Brown"
date: "4/21/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(igraph)
library(ggraph)
library(purrr)
library(stringr)
library(DT)
library(gridExtra)
library(ggpubr)
# library(statnet)
# library(tnet)
# library(intergraph)

```

# Sensitivity Analysis
Here, we run a variety of sensitivity analyses on the original and time-averaged networks. This code has been adapted from Peeples, Matthew A. 2017. Network Science and Statistical Techniques for Dealing with Uncertainties in Archaeological Datasets. [online]. Available: [www.mattpeeples.net/netstats.html](http://www.mattpeeples.net/netstats.html). Here, we use igraph rather than the sna and network packages.

Currently, most of this RMD focuses only on 1 Chaco dataset (i.e., AD 800). This script now fits into the workflow with the rest of the Chaco scripts. However, this script still needs to be updated to apply to all of the networks rather than just the 1 subset (i.e., the original AD 800 network).

## Load and prepare data for sensitivity analysis.

```{r load_data}

# First, load in all of the the igraph objects (i.e., Chaco_original_graphs and Chaco_ta_graphs). However, we only need the second one, as these will include the original graphs too.
load("../Data/Chaco_graph_objects.RData")

# The lists are unnested for 1 level.
Chaco.nets <- purrr::map(Chaco_ta_graphs, 1)
# Give names to each list of time period to keep track.
chaco.names <- c("chaco800", "chaco825", "chaco850", "chaco875", 
              "chaco900", "chaco925", "chaco950", "chaco975",
              "chaco1000", "chaco1025", "chaco1050", "chaco1075",
              "chaco1100", "chaco1125", "chaco1150", "chaco1175",
              "chaco1200", "chaco1225", "chaco1250", "chaco1275")
names(Chaco.nets) <- chaco.names

# Also, further prepare a name list that will be used to rename elements in a list for plotting.
chaco.net.names <- as.data.frame(chaco.names) %>%
  replicate(n = length(Chaco.nets), simplify = FALSE) %>% 
  unlist() %>% 
  as.data.frame() %>%
  dplyr::rename(period = 1)
chaco.net.names$period <-
  factor(chaco.net.names$period, levels = chaco.names)
chaco.net.names <-
  chaco.net.names[order(chaco.net.names$period), ] %>% 
  as.data.frame() %>% 
  dplyr::rename(period = 1)
chaco.net.names$period <- as.character(chaco.net.names$period)

network.names <- as.data.frame(paste("network", 1:length(Chaco.nets), sep = "_")) %>% 
  dplyr::rename(network = 1)

chaco.net.names.full <- cbind(chaco.net.names, network.names)
chaco.net.names.full <- chaco.net.names.full %>% 
  tidyr::unite(period_network, c("period", "network"))


```

## Functions to get multiple centrality scores for binary networks.
```{r centrality, echo=FALSE}

# Calculate centrality scores for binary networks for igraph objects.
net.stats <- function(y) {
    # Calculate degree centrality
    dg <- as.matrix(igraph::degree(y))
    # Calculate eigenvector centrality
    eg <- as.matrix(igraph::evcent(y)$vector)
    #eg <- sqrt((eg^2) * length(eg)) #scale centrality
    # Calculate betweenness centrality
    bw <- igraph::betweenness(y, normalized = TRUE, directed = FALSE)
    # Combine centrality scores into matrix
    output <- cbind(dg, eg, bw)
    colnames(output) <- c("dg", "eg", "bw")
    return(output)
}  # return results of this function

net.stats.single <- function(y) {
    # Calculate clustering coefficient
    cc <- igraph::transitivity(y, "global")
    # cc.matrix <- matrix(c(cc,rep(0, nrow(dg)-1)), nrow = nrow(dg), ncol = 1)
    # cc <- as.matrix(rep(cc.matrix[1,], nrow(cc.matrix)))
    # Calculate modularity
    wtc = igraph::cluster_walktrap(y)
    mod <- igraph::modularity(as.undirected(y), membership(wtc))
    # mod.matrix <- matrix(c(mod,rep(0, nrow(dg)-1)), nrow = nrow(dg), ncol = 1)
    # mod <- as.matrix(rep(mod.matrix[1,], nrow(mod.matrix)))
    # Combine centrality scores into matrix
    output <- cbind(cc, mod)
    colnames(output) <- c("cc", "mod")
    return(output)
}  # return results of this function

```

## Functions for creating outputs
```{r output_functions, echo = FALSE, warning=F, message=F}
# Create output table for sensitivity analysis
create_output_tables = function(BR.list) {
  dg.df = data.frame()
  eg.df = data.frame()
  bt.df = data.frame()
  cc.df = data.frame()
  mod.df = data.frame()
  
  for(n in chaco.names) {
    for(ta in 1:length(BR.list[n][[1]])) {
      dg = as.data.frame(BR.list[n][[1]][[ta]][[1]]) %>% 
        tidyr::gather("S90","S80","S70","S60","S50","S40","S30",
                      "S20","S10",key = sample.perc,value = dg.rho) %>% 
        dplyr::mutate(sample.perc = as.numeric(str_sub(sample.perc, start = 2, end = length(sample.perc)))) %>% 
        dplyr::arrange(sample.perc) %>%
        dplyr::mutate(num.net = ta) %>%
        dplyr::mutate(orig.net = n)
      dg.df = rbind(dg.df, dg)
      
      eg = as.data.frame(BR.list[n][[1]][[ta]][[2]]) %>% 
        tidyr::gather("S90","S80","S70","S60","S50","S40","S30",
                      "S20","S10",key = sample.perc,value = eg.rho) %>% 
        dplyr::mutate(sample.perc = as.numeric(str_sub(sample.perc, start = 2, end = length(sample.perc)))) %>% 
        dplyr::arrange(sample.perc) %>%
        dplyr::mutate(num.net = ta) %>%
        dplyr::mutate(orig.net = n)
      eg.df = rbind(eg.df, eg)
      
      bt = as.data.frame(BR.list[n][[1]][[ta]][[3]]) %>% 
        tidyr::gather("S90","S80","S70","S60","S50","S40","S30",
                      "S20","S10",key = sample.perc,value = bt.rho) %>% 
        dplyr::mutate(sample.perc = as.numeric(str_sub(sample.perc, start = 2, end = length(sample.perc)))) %>% 
        dplyr::arrange(sample.perc) %>%
        dplyr::mutate(num.net = ta) %>%
        dplyr::mutate(orig.net = n)
      bt.df = rbind(bt.df, bt)
      
      cc = as.data.frame(BR.list[n][[1]][[ta]][[4]]) %>% 
        tidyr::gather("S100","S90","S80","S70","S60","S50","S40","S30",
                      "S20","S10",key = sample.perc,value = cc.val) %>% 
        dplyr::mutate(sample.perc = as.numeric(str_sub(sample.perc, start = 2, end = length(sample.perc)))) %>% 
        dplyr::arrange(sample.perc) %>%
        dplyr::mutate(num.net = ta) %>%
        dplyr::mutate(orig.net = n)
      cc.df = rbind(cc.df, cc)
      
      mod = as.data.frame(BR.list[n][[1]][[ta]][[5]]) %>% 
        tidyr::gather("S100","S90","S80","S70","S60","S50","S40","S30",
                      "S20","S10",key = sample.perc,value = mod.val) %>% 
        dplyr::mutate(sample.perc = as.numeric(str_sub(sample.perc, start = 2, end = length(sample.perc)))) %>% 
        dplyr::arrange(sample.perc) %>%
        dplyr::mutate(num.net = ta) %>%
        dplyr::mutate(orig.net = n)
      mod.df = rbind(mod.df, mod)
    }
  }
  
  output.dg = dg.df %>% group_by(orig.net, num.net, sample.perc) %>%
    summarize(
      min = min(dg.rho, na.rm = TRUE), 
      quart.1 = quantile(dg.rho, probs = 0.25, na.rm = TRUE),
      med = median(dg.rho, na.rm = TRUE),
      quart.3 = quantile(dg.rho, probs = 0.75, na.rm = TRUE),
      max = max(dg.rho, na.rm = TRUE)
    ) %>% 
    dplyr::mutate(orig.net = factor(orig.net, levels = chaco.names)) %>%
    dplyr::arrange(orig.net)
  
  output.eg = eg.df %>% group_by(orig.net, num.net, sample.perc) %>%
    summarize(
      min = min(eg.rho, na.rm = TRUE), 
      quart.1 = quantile(eg.rho, probs = 0.25, na.rm = TRUE),
      med = median(eg.rho, na.rm = TRUE),
      quart.3 = quantile(eg.rho, probs = 0.75, na.rm = TRUE),
      max = max(eg.rho, na.rm = TRUE)
    ) %>% 
    dplyr::mutate(orig.net = factor(orig.net, levels = chaco.names)) %>%
    dplyr::arrange(orig.net)
  
  output.bt = bt.df %>% group_by(orig.net, num.net, sample.perc) %>%
    summarize(
      min = min(bt.rho, na.rm = TRUE), 
      quart.1 = quantile(bt.rho, probs = 0.25, na.rm = TRUE),
      med = median(bt.rho, na.rm = TRUE),
      quart.3 = quantile(bt.rho, probs = 0.75, na.rm = TRUE),
      max = max(bt.rho, na.rm = TRUE)
    ) %>% 
    dplyr::mutate(orig.net = factor(orig.net, levels = chaco.names)) %>%
    dplyr::arrange(orig.net)
  
  output.cc = cc.df %>% group_by(orig.net, num.net, sample.perc) %>%
    summarize(
      min = min(cc.val, na.rm = TRUE), 
      quart.1 = quantile(cc.val, probs = 0.25, na.rm = TRUE),
      med = median(cc.val, na.rm = TRUE),
      quart.3 = quantile(cc.val, probs = 0.75, na.rm = TRUE),
      max = max(cc.val, na.rm = TRUE)
    ) %>% 
    dplyr::mutate(orig.net = factor(orig.net, levels = chaco.names)) %>%
    dplyr::arrange(orig.net)
  
  output.mod = mod.df %>% group_by(orig.net, num.net, sample.perc) %>%
    summarize(
      min = min(mod.val, na.rm = TRUE), 
      quart.1 = quantile(mod.val, probs = 0.25, na.rm = TRUE),
      med = median(mod.val, na.rm = TRUE),
      quart.3 = quantile(mod.val, probs = 0.75, na.rm = TRUE),
      max = max(mod.val, na.rm = TRUE)
    ) %>% 
    dplyr::mutate(orig.net = factor(orig.net, levels = chaco.names)) %>%
    dplyr::arrange(orig.net)
  
  return(list(output.dg, output.eg, output.bt, output.cc, output.mod))
}
```


## Potential impact of missing nodes

### Over multiple centrality measures and/or networks

```{r missing_nodes_multiple, echo=FALSE, warning=FALSE}

# Set number of replicates (keeping number low for now, so that it will run quicker, but should definitely increase this number to ?10,000 for better results).
nsim <- 2
samp.frac <-
  c("S90", "S80", "S70", "S60", "S50", "S40", "S30", "S20", "S10")

# This function is for a binary network. This function does the same calculation as above but is set up to work with the output of net.stats.
cv.resamp.bin <- function(x) {
  # Calculate all network stats for the original network.
  stats.g <- net.stats(x)
  stats.g.single <- net.stats.single(x)
  mat <- as.matrix(igraph::get.adjacency(x))
  # Count number of rows (nodes).
  dim.x <- dim(mat)[1]
  # Define empty matrices for output.
  dg.mat <- matrix(NA, nsim, 9)
  ev.mat <- matrix(NA, nsim, 9)
  bw.mat <- matrix(NA, nsim, 9)
  cc.mat <- matrix(NA, 1, 9)
  mod.mat <- matrix(NA, 1, 9)
  # Add column names based on sampling fraction.
  colnames(dg.mat) <- samp.frac
  colnames(ev.mat) <- samp.frac
  colnames(bw.mat) <- samp.frac
  colnames(cc.mat) <- samp.frac
  colnames(mod.mat) <- samp.frac
  
  # This double loop goes through each sampling fraction and each random replicate to calculate centrality statistics and runs a Spearman's Rho correlation between the resulting centrality values and the original sample. However, for clustering coefficient and modularity, these will just output the values from the different networks.
  for (j in 1:9) {
    for (i in 1:nsim) {
      sub.samp <-
        sample(seq(1, dim.x),
               size = round(dim.x * ((10 - j) / 10),
                            0),
               replace = F)
      temp.stats <-
        net.stats(igraph::graph_from_adjacency_matrix(mat[sub.samp, sub.samp]))
      temp.stats.single <-
        net.stats.single(igraph::graph_from_adjacency_matrix(mat[sub.samp, sub.samp]))
      dg.mat[i, j] <-
        suppressWarnings(cor(temp.stats[, 1], stats.g[sub.samp,
                                                      1], method = "spearman"))
      ev.mat[i, j] <-
        suppressWarnings(cor(temp.stats[, 2], stats.g[sub.samp,
                                                      2], method = "spearman"))
      bw.mat[i, j] <-
        suppressWarnings(cor(temp.stats[, 3], stats.g[sub.samp,
                                                      3], method = "spearman"))
      cc.mat[1, j] <- temp.stats.single[, 1]
      mod.mat[1, j] <- temp.stats.single[, 2]
    }
  }
  # Add original to cc.mat and mod.mat (i.e,. 100% sampling).
  cc.mat <- as.data.frame(cc.mat) %>%
    dplyr::mutate(S100 = stats.g.single[1]) %>%
    dplyr::select(S100, everything()) %>%
    as.matrix()
  mod.mat <- as.data.frame(mod.mat) %>%
    dplyr::mutate(S100 = stats.g.single[2]) %>%
    dplyr::select(S100, everything()) %>%
    as.matrix()
  # Create list for output and populate it.
  out.list <- list()
  out.list[[1]] <- dg.mat
  out.list[[2]] <- ev.mat
  out.list[[3]] <- bw.mat
  out.list[[4]] <- cc.mat
  out.list[[5]] <- mod.mat
  
  return(out.list)
}  # return the resulting list


# Now apply the function to all 400 networks (20 per time period, which is for the 20 time averaged Chaco networks) for Chaco.
BR.rs <- Chaco.nets %>% purrr::map( ~ purrr::map(.x, cv.resamp.bin))


# Create output tables for node removal sensitivity analysis
outputs.rs = create_output_tables(BR.rs)
# knitr::kable(outputs.rs[[1]], caption = "Degree Centrality Rho Values")
# knitr::kable(outputs.rs[[2]], caption = "Eigenvector Centrality Rho Values")
# knitr::kable(output.rs[[3]], caption = "Betweenness Centrality Rho Values")
# knitr::kable(output.rs[[4]], caption = "Clustering Coefficient Values")
# knitr::kable(output.rs[[5]], caption = "Modularity Values")
datatable(outputs.rs[[1]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Degree Centrality Rho Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.rs[[2]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Eigenvector Centrality Rho Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.rs[[3]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Betweenness Centrality Rho Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.rs[[4]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Clustering Coefficient Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.rs[[5]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Modularity Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)



# Plotting
# Here, all the networks are unlisted. The final result is that there is the year of the network, then the final digits are the network number. For example, for the first time averaged network for AD 800, it would be listed as "chaco8001", then "chaco8002" and all the way to "chaco80020". Then, this would continue all the way to the 400th network, which would be "chaco125020".
BR.rs.plotting <- unlist(BR.rs,recursive=FALSE)

# This outputs a list of ggplot objects for all the time averaged networks for Chaco, using the plot_sa function (in R folder).
plots_sa_nodes <- BR.rs.plotting %>% purrr::map(plot_sa)

# Then, can rename each plot to have network number along with year.
names(plots_sa_nodes) <- chaco.net.names.full

```

### Node specific assessment
Here, we determine whether the most central node is consistently the most central.

```{r node_specific, echo=FALSE, include = F, warning=F, eval=F}

nsim <- 2 #set number of replicates

resamp.node <- function(x, samp.frac) {
  mat <- as.matrix(igraph::get.adjacency(x))
  dim.x <- dim(mat)[1]
  out.mat <- matrix(NA, dim.x, nsim)
  for (i in 1:nsim) {
    sub.samp <-
      sample(seq(1, dim.x),
             size = round(dim.x * samp.frac, 0),
             replace = F)
    # Calculate centrality statistic for a given sub-sample and put in output matrix.
    temp.stats <-
      igraph::degree(igraph::graph_from_adjacency_matrix(mat[sub.samp, sub.samp]), normalized = TRUE)
    out.mat[sub.samp, i] <- temp.stats
  }
  return(out.mat)
}


# Calculate the rank order of degree centrality scores in the original network.
top.dg <- rank(-igraph::degree(tn), ties.method = "min")

# Call the resamp.node function with a sampling fraction of 80% (samp.frac=0.8). Then, produce a barplot of the results for the first 4 nodes from the original network.
BR.resamp <- resamp.node(tn, samp.frac = 0.8)  #samp.frac is 80%
par(mfrow = c(2, 3))
# Calculate the rank order of the replicates and plot the top 4 as barplots showing rank across all replicates.
for (i in 1:5) {
  barplot(table(
    apply(
      -BR.resamp,
      2,
      rank,
      ties.method = "random",
      na.last = "keep"
    )[order(top.dg)[i],]
  ), main = paste("samp.frac=80%, rank = ", top.dg[order(top.dg)[i]]))
}

# Now, try with a sampling fraction of 50%.
BR.resamp <- resamp.node(tn, samp.frac = 0.5)  #samp.frac is 50%
par(mfrow = c(2, 3))
# Calculate the rank order of the replicates and plot the top 4 as barplots showing rank across all replicates.
for (i in 1:5) {
  barplot(table(
    apply(
      -BR.resamp,
      2,
      rank,
      ties.method = "random",
      na.last = "keep"
    )[order(top.dg)[i],]
  ),
  main = paste("samp.frac=50%, original rank = ", top.dg[order(top.dg)[i]]))
}

par(mfrow = c(1, 1))

resamp.bw <- function(x, samp.frac) {
  mat <- as.matrix(igraph::get.adjacency(x))
  dim.x <- dim(mat)[1]
  out.mat <- matrix(NA, dim.x, nsim)
  for (i in 1:nsim) {
    sub.samp <-
      sample(seq(1, dim.x),
             size = round(dim.x * samp.frac, 0),
             replace = F)
    temp.stats <-
      igraph::betweenness(igraph::graph_from_adjacency_matrix(mat[sub.samp, sub.samp]), normalized = TRUE)
    out.mat[sub.samp, i] <- temp.stats
  }
  return(out.mat)
}

# Calculate the rank order of betweenness centrality in the original network.
top.bw <- rank(-igraph::betweenness(tn), ties.method = "min")
BR.resamp <- resamp.bw(tn, samp.frac = 0.5)

par(mfrow = c(2, 3))
for (i in 1:5) {
  barplot(table(
    apply(
      -BR.resamp,
      2,
      rank,
      ties.method = "random",
      na.last = "keep"
    )[order(top.dg)[i],]
  ), main = paste("samp.frac=50%, rank = ", top.bw[order(top.bw)[i]]))
}

```


## Potential impact of missing edges
```{r missing_edges, echo=FALSE, warning=F, message=F}

# Create a function that calculates correlations between the original and sub-sampled replicates from 90% to 10% for all three of the primary measures of centrality and clustering coefficient and modularity using the format from the net.stats and net.stats.single function to assess the rank order correlation of nodes.
nsim <- 2
samp.frac <-
  c("S90", "S80", "S70", "S60", "S50", "S40", "S30", "S20", "S10")

cv.resamp.edge <- function(x) {
  stats.g <- net.stats(x)
  stats.g.single <- net.stats.single(x)
  mat <- as.matrix(igraph::get.adjacency(x))
  dim.x <- dim(mat)[1]
  dg.mat <- matrix(NA, nsim, 9)
  ev.mat <- matrix(NA, nsim, 9)
  bw.mat <- matrix(NA, nsim, 9)
  cc.mat <- matrix(NA, 1, 9)
  mod.mat <- matrix(NA, 1, 9)
  colnames(dg.mat) <- samp.frac
  colnames(ev.mat) <- samp.frac
  colnames(bw.mat) <- samp.frac
  colnames(cc.mat) <- samp.frac
  colnames(mod.mat) <- samp.frac
  
  for (j in 1:9) {
    for (i in 1:nsim) {
      sub.samp <-
        sample(
          seq(1, igraph::gsize(x)),
          size = round(igraph::gsize(x) *
                         (j / 10), 0),
          replace = F
        )
      temp.net <- x
      net.reduced <- igraph::delete.edges(temp.net, sub.samp)
      temp.stats <- net.stats(net.reduced)
      temp.stats.single <- net.stats.single(net.reduced)
      dg.mat[i, j] <-
        cor(temp.stats[, 1], stats.g[, 1], method = "spearman")
      ev.mat[i, j] <-
        cor(temp.stats[, 2], stats.g[, 2], method = "spearman")
      bw.mat[i, j] <-
        cor(temp.stats[, 3], stats.g[, 3], method = "spearman")
      cc.mat[1, j] <- temp.stats.single[, 1]
      mod.mat[1, j] <- temp.stats.single[, 2]
      
    }
  }
  # Add original to cc.mat and mod.mat (i.e., 100% sampling).
  cc.mat <- as.data.frame(cc.mat) %>%
    dplyr::mutate(S100 = stats.g.single[1]) %>%
    dplyr::select(S100, everything()) %>%
    as.matrix()
  mod.mat <- as.data.frame(mod.mat) %>%
    dplyr::mutate(S100 = stats.g.single[2]) %>%
    dplyr::select(S100, everything()) %>%
    as.matrix()
  
  out.list <- list()
  out.list[[1]] <- dg.mat
  out.list[[2]] <- ev.mat
  out.list[[3]] <- bw.mat
  out.list[[4]] <- cc.mat
  out.list[[5]] <- mod.mat
  return(out.list)
}

# Now apply the function to all 400 networks (20 per time period) for Chaco.
BR.edge <- Chaco.nets %>%
  purrr::map( ~ purrr::map(.x, cv.resamp.edge))


# Create output tables for node removal sensitivity analysis
outputs.edge = create_output_tables(BR.edge)
# knitr::kable(outputs.edge[[1]], caption = "Degree Centrality Rho Values")
# knitr::kable(outputs.edge[[2]], caption = "Eigenvector Centrality Rho Values")
# knitr::kable(output.edge[[3]], caption = "Betweenness Centrality Rho Values")
# knitr::kable(output.edge[[4]], caption = "Clustering Coefficient Values")
# knitr::kable(output.edge[[5]], caption = "Modularity Values")
datatable(outputs.edge[[1]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Degree Centrality Rho Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.edge[[2]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Eigenvector Centrality Rho Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.edge[[3]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Betweenness Centrality Rho Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.edge[[4]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Clustering Coefficient Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)
datatable(outputs.edge[[5]], extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('copy', 'csv')), caption = "Modularity Values", rownames = FALSE, colnames = c("Original Network", "Number of Networks Averaged", "Sample Percentage", "Min", "1st Quartile", "Median", "3rd Quartile", "Max")) %>%
  formatRound(c("min", "quart.1", "med", "quart.3", "max"), 3)


# Plotting
# Here, all the networks are unlisted. The final result is that there is the year of the network, then the final digits are the network number. For example, for the first time averaged network for AD 800, it would be listed as "chaco8001", then "chaco8002" and all the way to "chaco80020". Then, this would continue all the way to the 400th network, which would be "chaco125020".
BR.edge.plotting <- unlist(BR.edge,recursive=FALSE)

# This outputs a list of ggplot objects for all the time averaged networks for Chaco, using the plot_sa function (in R folder).
plots_sa_edges <- BR.edge.plotting %>% 
  purrr::map(plot_sa)

# Then, can rename each plot to have network number along with year.
names(plots_sa_edges) <- chaco.net.names.full

```